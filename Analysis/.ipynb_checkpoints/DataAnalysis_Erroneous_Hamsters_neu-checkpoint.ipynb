{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To err is human, but where is the error?\n",
    "### Erronous Hamsters group project\n",
    "### Analysis of the HCP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ClaraQ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: \n",
      "\n",
      " | Starting with Nilearn 0.7.0, all Nistats functionality has been incorporated into Nilearn's stats & reporting modules.\n",
      " | Nistats package will no longer be updated or maintained.\n",
      "\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#load important libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nistats.hemodynamic_models import glover_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HCP_DIR =  \"c:/users/frauke/downloads/hcp_task\" #change to your data path\n",
    "HCP_DIR =  \"C:\\Users\\ClaraQ\\Documents\\Projects\\Neuromatch_ErrorTypes\\Data\\hcp_task\" \n",
    "\n",
    "\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and update necessary files and subject-folders\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excl_no_error_sub(files_oI, subjects, HCP_DIR):\n",
    "    \n",
    "    \"\"\"Get list of subject with 0 errors in at least one condition \n",
    "       in task: \"tfMRI_WM_RL\", \"tfMRI_WM_LR\".\n",
    "\n",
    "    Args:\n",
    "      files_oI (list of strings) : list of condition .txt files to check ()\n",
    "      subjects (list of int) : subjects t\n",
    "      \n",
    "    Returns:\n",
    "       2x excl_subs_**** (tuple) : list of unique subjects\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "\n",
    "\n",
    "    files_oI = files_oI        #currently ['2bk_err.txt', '2bk_nlr.txt']\n",
    "\n",
    "    excl_subs_tfMRI_WM_RL = [] ##initialize list per task \n",
    "    excl_subs_tfMRI_WM_LR = []\n",
    "\n",
    "\n",
    "    for t in [\"tfMRI_WM_RL\", \"tfMRI_WM_LR\"]: # iterate over task and subjects\n",
    "\n",
    "        for sub in subjects:\n",
    "\n",
    "            path = HCP_DIR + \"/subjects/{}/EVs/{}\".format(sub,t) #set path\n",
    "            files = os.listdir(path)\n",
    "            files = [file for file in files if file in files_oI] # choose files of interest from path\n",
    "\n",
    "            for file in files:\n",
    "                f = np.genfromtxt(path + \"/\" + file)\n",
    "\n",
    "                if f.size == 0: # check for empty files in specific task\n",
    "                    if t == \"tfMRI_WM_RL\":\n",
    "                        excl_subs_tfMRI_WM_RL.append(sub) # append to respective list \n",
    "                    else:\n",
    "                        excl_subs_tfMRI_WM_LR.append(sub)\n",
    "                        \n",
    "    excl_subj = list(set(excl_subs_tfMRI_WM_RL + excl_subs_tfMRI_WM_LR))\n",
    "    subj_all = [sub for sub in subjects if sub not in excl_subj]\n",
    "    \n",
    "    # return unique subjects per task, hint: if files_oI contains > 1 condition, doubles are possible\n",
    "    \n",
    "    return subj_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(data, subject, bold_run, HCP_DIR, taskcondition):\n",
    "    '''Write txt files from arrays, no matter whether they are empty\n",
    "    Args:\n",
    "        data: data to write in file\n",
    "        subject(int): name of subject\n",
    "        bold_run(str): name of the bold_run/task, e.g. wm_rl\n",
    "        HCP_DIR(str): path which contains data\n",
    "        taskcondition(str): new name of error/condition\n",
    "    '''\n",
    "    \n",
    "    # Check data for size\n",
    "    if data.size < 3: #if an error was made there would be at least one row for onset,duration,amplitude --> 3 int\n",
    "        data = [] #set data equals empty\n",
    "        #write an empty file in subjects folder of the the task with the errorcondition name\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/{taskcondition}.txt\",\n",
    "               data, delimiter='\\t')\n",
    "    else: \n",
    "        if data.size == 3: #if data contains 3 columns\n",
    "            data = data.reshape(1,3)\n",
    "        #write file which contains data\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/{taskcondition}.txt\",\n",
    "               data, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "        \n",
    "def remove_omission_errors(HCP_DIR, subjects, bold_run, errconditions, nlrconditions):\n",
    "    '''Remove omission errors from _bk_err.txt file.\n",
    "\n",
    "    Args:\n",
    "      HCP_DIR(str) : Path to target directory\n",
    "      subjects(list): list of all subjects whos error files should be updated\n",
    "      bold_run(list): list of task whos error files should be updated\n",
    "      errconditions(list): list of error files which should be updated\n",
    "      nlrconditions(list): list of nullresponse files which are used for comparison between false response and nullresponse\n",
    "    Returns:\n",
    "      New files '_bk_err_fresp.txt' in source directory \n",
    "  '''\n",
    "    for subject in subjects: \n",
    "        for run in bold_run:\n",
    "            #Load error files as numpy arrays \n",
    "            nlrconcat = np.zeros([1,3])\n",
    "            ONSETS = []\n",
    "            #loop through all errorconditions\n",
    "            for i in np.arange(len(errconditions)):\n",
    "                #print('subject {0}, task key {1}, i = {2}'.format(subject, task_key, i))\n",
    "                errorcondition = errconditions[i] #set actual error condition\n",
    "                err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{errorcondition}.txt\") #load corresponding error file\n",
    "                nlr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{nlrconditions[i]}.txt\") #load corresponding null response file\n",
    "                errorname = str(errconditions[i].split('_')[0]+'_fresp') #create the name of sorted error as false response\n",
    "                #if there were zero null responses\n",
    "                if nlr.size == 0:\n",
    "                    #the complete error file contains just false reponses, so the data to write in new file equals the errorfile\n",
    "                    data = err\n",
    "                    #write new error- false reponse file\n",
    "                    write_files(data, subject, run, HCP_DIR, errorname)\n",
    "                    #go to next condition\n",
    "                    continue\n",
    "                # if there was one null reponse AND just one error in the other error file\n",
    "                elif nlr.ndim == 1 and err.ndim == 1:\n",
    "                    #those error file just contains a null response which should be deleted\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[0]\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0)\n",
    "                    #data = []\n",
    "                    write_files(data, subject, run, HCP_DIR, errorname)\n",
    "                # if there was just one null response but more total errors\n",
    "                elif nlr.ndim == 1 and err.ndim > 1:\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[:,0] # create mask for null reponse in other errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, run, HCP_DIR, errorname)\n",
    "                #else if there were more than one null response and more than one total errors\n",
    "                else:\n",
    "                    onsets = nlr[:,0]\n",
    "                    mask = np.isin(err, nlr)[:,0] #create mask for null response in total errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, run, HCP_DIR, errorname)\n",
    "                ONSETS = np.append(ONSETS, onsets)\n",
    "\n",
    "            all_err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_bk_err.txt\")\n",
    "            errorcondition = 'all_bk_err'\n",
    "            errorname = str(errorcondition.split('_')[0]+'_fresp')\n",
    "            #if there was no error at all\n",
    "            if all_err.size == 0:\n",
    "                data = all_err #false response data is empty as well\n",
    "                write_files(data, subject, run, HCP_DIR, errorname)\n",
    "                continue\n",
    "            #if amount of total errors equals 1\n",
    "            elif all_err.ndim == 1:\n",
    "                mask = np.isin(all_err[0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, run, HCP_DIR, errorname)\n",
    "            #if amount of total errors are more than 1\n",
    "            else:\n",
    "                mask = np.isin(all_err[:,0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, run, HCP_DIR, errorname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_X_sorted(HCP_DIR, subject, bold_run):\n",
    "    '''Loads _cor.txt and _err.txt files, concatenates and sorts after time\n",
    "    \n",
    "    Args:\n",
    "    - HCP_DIR (str)  : path of directory \n",
    "    - subject (int)  : current subject \n",
    "    - bold_run (str)  : task/bold run\n",
    "    \n",
    "    Returns: \n",
    "    - X (np.ndarray) : 80x2 ndarray with columns time stamp and errors (1 = error)\n",
    "    '''\n",
    "    #load all responses in two variables\n",
    "    corr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_cor.txt\")\n",
    "    err =  np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_err.txt\")\n",
    "    #stack all onset times nebeneinander\n",
    "    corr = np.hstack((corr[:,0].reshape(-1,1), np.zeros_like(corr[:,0].reshape(-1,1))))\n",
    "    if err.size == 3:\n",
    "        err = err[np.newaxis, :]\n",
    "    err = np.hstack((err[:,0].reshape(-1,1), np.ones_like(err[:,0]).reshape(-1,1)))\n",
    "    #stack both responses (correct and error) into one array\n",
    "    X = np.vstack((corr, err))\n",
    "    #sort onset times after size\n",
    "    X = X[X[:,0].argsort()]\n",
    "    \n",
    "    return X \n",
    "\n",
    "def diff_stimuli_types(HCP_DIR, subjects, stimuli, bold_run, task_keys):\n",
    "    '''\n",
    "        Takes all stimuli and sort them after stimuli type. Writes onset times of different stimuli type in a several files.\n",
    "        \n",
    "        Args:\n",
    "            HCP_DIR (str): directory of data\n",
    "            subjects(array): array of all subjects\n",
    "            stimuli(list): list of stimuli for which onset files should be written\n",
    "            bold_run(list): list of all run/task for which onset files for different stimuli should be written\n",
    "            task_keys(list): list of tasks for which onset files for different stimuli should be written\n",
    "        saves new stimuli onset files\n",
    "    '''\n",
    "    cue_duration = 2.565\n",
    "    len_block = 10\n",
    "    temp_zeros = np.zeros([80,4])\n",
    "    for subject in subjects:\n",
    "        for run in bold_run:\n",
    "            X = return_X_sorted(HCP_DIR=HCP_DIR, subject=subject, bold_run=run)\n",
    "            \n",
    "            for task_key in task_keys:\n",
    "                temp_zeros = np.zeros([80,4])\n",
    "                for index, stimulus in enumerate(stimuli):\n",
    "                    stim_name = task_key + stimulus\n",
    "                    stim = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}.txt\")\n",
    "                    #Search for index of start value (min distance between stim onset + \n",
    "                    # cue duration and start of block in onset times of X)\n",
    "                    min_index = np.argmin(np.abs(X[:,0] - (stim[0] + cue_duration)))\n",
    "                    # Create new column of zeros and ones \n",
    "                    temp_zeros[min_index:min_index+len_block, index] = 1\n",
    "                    T = np.hstack((X[:,0].reshape(-1,1), temp_zeros))\n",
    "\n",
    "                    #Create new file with time stamps \n",
    "                    M = np.hstack((X[min_index : min_index+len_block, 0].reshape(-1,1), np.full((len_block,1),2),\n",
    "                                   np.full((len_block, 1), 1)))\n",
    "                    # Optional if we want to include the first line\n",
    "                    stim[1] = 2.0\n",
    "                    M = np.vstack((stim, M))\n",
    "                    # Write file in source directory\n",
    "                    np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}_ts.txt\",\n",
    "                               M, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "                \n",
    "                #Write T into file\n",
    "                #np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_{task_key}ts.txt\",\n",
    "                #            T, fmt=['%.3f','%.d', '%.d','%.d', '%.d'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178.755,  27.5  ,   1.   ])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stim = np.genfromtxt(f\"{HCP_DIR}/subjects/0/EVs/tfMRI_WM_LR/2bk_places.txt\")\n",
    "stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "def load_evs(subject, name, condition):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id-1]\n",
    "    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], np.genfromtxt(ev_file).T))\n",
    "    evs.append(ev)\n",
    "  return evs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, bold_run, remove_mean=False):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "    \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / (TR/10)).astype(int)\n",
    "            \n",
    "        #Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / (TR/10)).astype(int)\n",
    "            \n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        if type(start) == np.ndarray:\n",
    "            frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "        else:\n",
    "              frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "        frames_list.append(np.concatenate(frames))\n",
    "        #print('ev-file is not empty')\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "def condition_frames_iti(run_evs, skip=0):\n",
    "    \"\"\"Identify starting timepoint and duration of an inter-trial-interval corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor((ev[\"onset\"]+2) / (TR/10)).astype(int) \n",
    "            \n",
    "        #Use trial duration to determine how many frames to include for trial        \n",
    "        duration = np.ceil(np.full(ev['duration'].size,0.5) / (TR/10)).astype(int)\n",
    "            \n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        if type(start) == np.ndarray:\n",
    "            frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "        else:\n",
    "            frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "        frames_list.append(np.concatenate(frames))\n",
    "\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and GLM-Fitting\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_design_matrix(subject,task,errconditions,conditions,sample_rate):\n",
    "    \"\"\"\n",
    "    Creates a design matrix for given tasks and conditions.\n",
    "    \n",
    "    Args:\n",
    "        task: string of specific task of experiment\n",
    "        errconditions(list): list of error files/conditions\n",
    "        conditions: list of conditions, which should be included in design matrix\n",
    "        sample_rate: int of how much the frames should be upsampled\n",
    "    \"\"\"\n",
    "    \n",
    "    #ignore warnings from genfromtxt\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "    #create instances of events and design_matrix\n",
    "    ts = load_single_timeseries(subject,get_image_ids(task)[0])\n",
    "    design_matrix = np.zeros((ts.shape[1]*sample_rate,(len(conditions)+len(errconditions))))\n",
    "    #loop through all conditions and set a 1 in design matrix for every time point condition is fullfilled\n",
    "    for n, cond in enumerate(errconditions):\n",
    "        frames= []\n",
    "        ev = [load_evs(subject,task,cond)]\n",
    "        frames = condition_frames_iti(ev[0],skip=0)\n",
    "        design_matrix[frames,n] = 1\n",
    "        \n",
    "    for s, condi in enumerate(conditions):\n",
    "        ev = [load_evs(subject,task,condi)]\n",
    "        frames = condition_frames(ev[0],skip=0)\n",
    "        design_matrix[frames,len(errconditions)+s] = 1\n",
    "    return design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrf_convo(design_matrix,canon_hrf):\n",
    "    \"\"\"\n",
    "    Takes designmatrix upsampled to TR/TReso and convolves it with canonical HRF\n",
    "    Args\n",
    "        design_matrix: C * VolTreso ndarray; indicates closest Tr/TReso timepoint where an event started for specific regressor\n",
    "        bf: basis function - will usually be the hrf; 1D array\n",
    "    Returns\n",
    "        design_matrix_conv: C*Vol array with predicted BOLD response\n",
    "    \"\"\"\n",
    "    design_matrix_pad=np.pad(design_matrix,((0,len(canon_hrf)),(0, 0))) # pad with zeros, so we can fit the hrf also for very late events\n",
    "    design_matrix_conv=np.empty(design_matrix_pad.shape) # prepare convolved design matrix\n",
    "    for C in range(len(design_matrix[1])): # for each condition C\n",
    "\n",
    "        design_matrix_conv[:,C]=np.convolve(design_matrix_pad[:len(design_matrix_pad[0])-(len(canon_hrf)+(design_matrix[0].size-1)),C],canon_hrf) #convolute with design with bf\n",
    "\n",
    "    design_matrix_downsampled=design_matrix_conv[:len(design_matrix_pad[0])-(len(canon_hrf)+design_matrix[0].size),:] # cut back the padded stuff\n",
    "\n",
    "    design_matrix_downsampled=design_matrix_downsampled[0::10,:] # pick every time-point where we have a volume\n",
    "    constant=np.ones((len(design_matrix_downsampled),1),dtype=int) # generate constant (intercept)\n",
    "    design_matrix_downsampled=np.append(design_matrix_downsampled,constant,axis=1) #append intercept to design matrix\n",
    "    return design_matrix_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_matrix[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some start variables\n",
    "# the shape of the design matrix is based on the script \"DataAnalysis_Errorss\"\n",
    "def test_orthogonality(design_matrix_downsampled, subject, threshold, warn = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes a design martix and computed the correlation between conditions\n",
    "    \n",
    "    input:\n",
    "    design_matrix = m x n np.array with condition columns and time line as a row\n",
    "    \n",
    "    output:\n",
    "    orth_mat = np.array of size n x n indicating the orthogonality of each condition to each other condition\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the output matrix\n",
    "    design_dim = design_matrix_downsampled.shape\n",
    "    orth_mat = np.zeros((3,3))\n",
    "    \n",
    "    # two vectors are orthogonal, if their dot product is zero\n",
    "    # loop through all conditions\n",
    "    for condition in range(0,3):\n",
    "        cond_1 = design_matrix_downsampled[:,condition]\n",
    "        # compare to all conditions\n",
    "        for compare in range(0,3):\n",
    "            cond_2 = design_matrix_downsampled[:,compare]\n",
    "            \n",
    "            orth_mat[condition,compare] = np.dot(cond_1,cond_2) / (np.linalg.norm(cond_1)*np.linalg.norm(cond_2))\n",
    "              \n",
    "    # throw a warning for conditions that are not orthogonal\n",
    "    check_pos = []\n",
    "    warn_orth = 0\n",
    "    if warn:\n",
    "        warnings = np.where(-threshold>orth_mat)\n",
    "        warnings = np.hstack((warnings,np.where(orth_mat>threshold)))\n",
    "        for pos in range(0,len(warnings[0])):\n",
    "            check_pos = np.zeros(len(warnings))\n",
    "\n",
    "            for dim in range(0,len(warnings)):\n",
    "                check_pos[dim] = warnings[dim][pos]\n",
    "\n",
    "            if not (check_pos[0]== check_pos[1]):\n",
    "                print('Watch out, the matrix is not orthogonal for conditons {} of subject:{}'.format(check_pos,subject))   \n",
    "                warn_orth +=1\n",
    "                \n",
    "    return orth_mat,check_pos, warn_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGLM(design_matrix_downsampled,TimeSeries):\n",
    "    \"\"\"\n",
    "    Takes design matrix and timeseries data and applies MLE to generate beta-estimates\n",
    "    Args: \n",
    "        design_matrix_GLM: C*Vol array with predicted BOLD signal per volume per regressor (MAY NOT CONTAIN 0-SUM VECTORS)\n",
    "        TimeSeries: Region*Vol array with actual BOLD signal averaged across regions\n",
    "    Return:\n",
    "        beta: \n",
    "    \"\"\"\n",
    "    X=design_matrix_downsampled\n",
    "    ts=TimeSeries # get time-series array\n",
    "    #ts=TimeSeries[0][0] # get time-series array\n",
    "    beta=np.empty((np.size(ts,axis=0),np.size(X,axis=1)))\n",
    "    for R in range(np.size(ts,axis=0)):\n",
    "        beta[R,:] = np.linalg.inv(X.T @ X) @ X.T @ ts[R,:].T\n",
    "    return beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level analysis \n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_beta_matrix():\n",
    "    '''\n",
    "        Takes \n",
    "    '''\n",
    "\n",
    "def contrast_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contrasts(beta,convecs):\n",
    "    \"\"\"\n",
    "    Multiplies beta estimates per subject with contrast vectors\n",
    "    Args:\n",
    "        beta: C x R array listing beta estimates for each region\n",
    "        convecs: matrix of contrasts, e.g.: [1 0 0 0 -1]\n",
    "    Returns\n",
    "        Con: Contrast-estimate\n",
    "    \"\"\"\n",
    "    \n",
    "    #convecs=np.array(convecs).reshape(-1,1)\n",
    "    Con=np.empty((np.size(beta,axis=0),np.size(convecs, axis=0)))\n",
    "    \n",
    "    for c in range(np.size(convecs, axis=0)):\n",
    "        for R in range(np.size(beta, axis=0)):\n",
    "            #print(np.dot(convecs, beta[R,:]))\n",
    "            Con[R,c]=np.dot(convecs[c,:].T, beta[R,:])\n",
    "    return Con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(contrast_all, threshold=2):\n",
    "    \"\"\" Checks for each region which subjects have outlying values (defined as exceeding a given threshold in standard deviations) \n",
    "    Args:\n",
    "        contrasts_all: Sub*R*contrasts matrix of contrast values\n",
    "        threshold: standard deviation limit indicating whether a participant is an outlier\n",
    "    Return\n",
    "        boocon: R*Sub array 0==outlier, 1==normal\n",
    "    \"\"\"\n",
    "    boocon=np.zeros_like(contrast_all)\n",
    "    for C in range(np.size(contrast_all, axis=2)):\n",
    "        for R in range(np.size(contrast_all, axis=1)):\n",
    "            boocon[:,R,C] = abs(contrast_all[:,R,C] - np.mean(contrast_all[:,R,C])) < (threshold * np.std(contrast_all[:,R,C]))\n",
    "    return boocon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize basic parameters\n",
    "bold_run = ['tfMRI_WM_RL','tfMRI_WM_LR']\n",
    "bold_ids = [get_image_ids('wm_rl')[0],get_image_ids('wm_lr')[0]]\n",
    "tasks = ['wm_rl','wm_lr']\n",
    "task_keys = ['0bk_','2bk_']\n",
    "stimuli = ['faces', 'body', 'tools', 'places']\n",
    "canon_hrf = glover_hrf(tr=TR,oversampling = 10)\n",
    "\n",
    "# #create error files with false response and null responses respectively\n",
    "# remove_omission_errors(HCP_DIR, subjects,bold_run, ['0bk_err','2bk_err'],['0bk_nlr','2bk_nlr'])\n",
    "\n",
    "# #initialize new error files\n",
    "# error_files = ['2bk_fresp.txt','2bk_nlr.txt']\n",
    "\n",
    "# #create lists of subjects with no error of each error type in both bold runs\n",
    "# subjects_all = excl_no_error_sub(error_files, subjects, HCP_DIR)\n",
    "# #subjects equals non-excluded subjects list\n",
    "# subjects = subjects_all\n",
    "\n",
    "# #create onset files for each specific stimuli type\n",
    "# diff_stimuli_types(HCP_DIR, subjects, stimuli, bold_run, task_keys)\n",
    "\n",
    "#loop through all subjects in one bold_run\n",
    "warn_subj = []\n",
    "for sub in subjects:\n",
    "    #make design matrix\n",
    "    design_matrix = make_design_matrix(sub,tasks[0],['2bk_fresp','2bk_nlr'],\n",
    "                                       ['2bk_cor','2bk_body_ts','2bk_faces_ts','2bk_places_ts','2bk_tools_ts'],10)\n",
    "    #convulate design matrix with hrf\n",
    "    design_matrix_downsampled = hrf_convo(design_matrix, canon_hrf)\n",
    "    #check for orthoganility between conditions\n",
    "    _,_,warns = test_orthogonality(design_matrix_downsampled, sub, threshold=0.4, warn = True)\n",
    "    if warns !=0:\n",
    "        warn_subj.append(sub) \n",
    "        continue\n",
    "    #load timeseries of subject for specific run\n",
    "    TimeSeries = load_single_timeseries(sub,bold_ids[0])\n",
    "    #estimate beta-weights\n",
    "    beta_weights = fitGLM(design_matrix_downsampled,TimeSeries)\n",
    "    #save beta-weights\n",
    "    np.savetxt(f\"{HCP_DIR}/subjects/{sub}/EVs/{bold_run[0]}/{task_keys[1]}_beta.txt\",beta_weights, delimiter=\",\")\n",
    "    \n",
    "subjects = [sub for sub in subjects if sub not in warn_subj]\n",
    "\n",
    "warn_subj = []\n",
    "for sub in subjects:\n",
    "    #make design matrix\n",
    "    design_matrix = make_design_matrix(sub,tasks[1],['2bk_fresp','2bk_nlr'],\n",
    "                                       ['2bk_cor','2bk_body_ts','2bk_faces_ts','2bk_places_ts','2bk_tools_ts'],10)\n",
    "    #convulate design matrix with hrf\n",
    "    design_matrix_downsampled = hrf_convo(design_matrix, canon_hrf)\n",
    "    #check for orthoganility between conditions\n",
    "    _,_,warns = test_orthogonality(design_matrix_downsampled, sub, threshold=0.4, warn = True)\n",
    "    if warns !=0:\n",
    "        warn_subj.append(sub)\n",
    "        continue\n",
    "    #load timeseries of subject for specific run\n",
    "    TimeSeries = load_single_timeseries(sub,bold_ids[1])\n",
    "    #estimate beta-weights\n",
    "    beta_weights = fitGLM(design_matrix_downsampled,TimeSeries)\n",
    "    #save beta-weights\n",
    "    np.savetxt(f\"{HCP_DIR}/subjects/{sub}/EVs/{bold_run[1]}/{task_keys[1]}_beta.txt\",beta_weights, delimiter=\",\")\n",
    "subjects = [sub for sub in subjects if sub not in warn_subj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define contrast\n",
    "contrast_fresp_nlr = [1,-1,0,0,0,0,0,0,1,-1,0,0,0,0,0,0]\n",
    "contrast_fresp_cor = [1,0,-1,0,0,0,0,0,1,0,-1,0,0,0,0,0]\n",
    "contrast_nlr_cor = [0,1,-1,0,0,0,0,0,0,1,-1,0,0,0,0,0]\n",
    "contrast_err_cor = [0.5,0.5,-1,0,0,0,0,0,0.5,0.5,-1,0,0,0,0,0]\n",
    "contrasts = np.vstack((contrast_fresp_nlr,contrast_fresp_cor,contrast_nlr_cor,contrast_err_cor))\n",
    "#init contrast-matrix for all subjects\n",
    "contrast_all = np.zeros((len(subjects),360,len(contrasts)))\n",
    "for n,sub in enumerate(subjects):\n",
    "    #load beta weights and stack them together\n",
    "    beta_rl = np.genfromtxt(f\"{HCP_DIR}/subjects/{sub}/EVs/{bold_run[0]}/{task_keys[1]}_beta.txt\",delimiter=\",\")\n",
    "    beta_lr = np.genfromtxt(f\"{HCP_DIR}/subjects/{sub}/EVs/{bold_run[1]}/{task_keys[1]}_beta.txt\",delimiter=\",\")\n",
    "    beta_all = np.hstack((beta_rl,beta_lr))\n",
    "    \n",
    "    contrast_all[n,:,:] = calculate_contrasts(beta_all,contrasts)\n",
    "\n",
    "outliers = find_outliers(contrast_all, threshold=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 16)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrasts.shape\n",
    "beta_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 360, 4)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
