{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Connectome Project (HCP) Dataset loader\n",
    "\n",
    "The HCP dataset comprises resting-state and task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified by Jonathan Orschiedt, uploaded to GitHub by Clara Kuper\n",
    "# Date: 21/07/2020\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCP_DIR =  \"./hcp_task\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading region information\n",
    "Downloading either dataset will create the regions.npy file, which contains the region name and network assignment for each parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './regions.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6d8f9147bd58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"./regions.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m region_info = dict(\n\u001b[0;32m      3\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmyelin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './regions.npy'"
     ]
    }
   ],
   "source": [
    "regions = np.load(f\"./regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    myelin=regions[2].astype(np.float),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load \n",
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
    "  ]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
    "      or None to load all runs.\n",
    "    concat (bool) : If True, concatenate multiple runs in time\n",
    "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  # Get the list relative 0-based index of runs to use\n",
    "  if runs is None:\n",
    "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
    "  elif isinstance(runs, int):\n",
    "    runs = [runs]\n",
    "\n",
    "  # Get the first (1-based) run id for this experiment \n",
    "  offset = get_image_ids(name)[0]\n",
    "\n",
    "  # Load each run's data\n",
    "  bold_data = [\n",
    "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
    "  ]\n",
    "\n",
    "  # Optionally concatenate in time\n",
    "  if concat:\n",
    "    bold_data = np.concatenate(bold_data, axis=-1)\n",
    "\n",
    "  return bold_data\n",
    "\n",
    "\n",
    "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"./subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "def load_evs(subject, name, condition):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id - 1]\n",
    "    ev_file = f\"./subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], np.genfromtxt(ev_file).T))\n",
    "    evs.append(ev)\n",
    "  return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_task = []\n",
    "for subject in subjects:\n",
    "  timeseries_task.append(load_timeseries(subject, \"motor\", concat=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "\n",
    "  Args:\n",
    "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "  for ev in run_evs:\n",
    "\n",
    "    # Determine when trial starts, rounded down\n",
    "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "\n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "\n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "\n",
    "    frames_list.append(np.concatenate(frames))\n",
    "\n",
    "  return frames_list\n",
    "\n",
    "\n",
    "def selective_average(timeseries_data, ev, skip=0):\n",
    "  \"\"\"Take the temporal mean across frames for a given condition.\n",
    "\n",
    "  Args:\n",
    "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
    "    ev (dict or list of dicts): Condition timing information\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    avg_data (1D array): Data averagted across selected image frames based\n",
    "    on condition timing\n",
    "\n",
    "  \"\"\"\n",
    "  # Ensure that we have lists of the same length\n",
    "  if not isinstance(timeseries_data, list):\n",
    "    timeseries_data = [timeseries_data]\n",
    "  if not isinstance(ev, list):\n",
    "    ev = [ev]\n",
    "  if len(timeseries_data) != len(ev):\n",
    "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
    "\n",
    "  # Identify the indices of relevant frames\n",
    "  frames = condition_frames(ev)\n",
    "\n",
    "  # Select the frames from each image\n",
    "  selected_data = []\n",
    "  for run_data, run_frames in zip(timeseries_data, frames):\n",
    "    selected_data.append(run_data[:, run_frames])\n",
    "\n",
    "  # Take the average in each parcel\n",
    "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
    "\n",
    "  return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"motor\"\n",
    "conditions = [\"rh\", \"t\"]  # Run a substraction analysis between two conditions\n",
    "\n",
    "contrast = []\n",
    "for subject in subjects:\n",
    " \n",
    "  # Get the average signal in each region for each condition\n",
    "  evs = [load_evs(subject, task, cond) for cond in conditions]\n",
    " \n",
    "  avgs = [selective_average(timeseries_task[subject], ev) for ev in evs]\n",
    "\n",
    "  # Store the region-wise difference\n",
    "  contrast.append(avgs[0] - avgs[1])\n",
    "\n",
    "group_contrast = np.mean(contrast, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi_contrasts = np.split(group_contrast, 2)\n",
    "\n",
    "for i, hemi_contrast in enumerate(hemi_contrasts):\n",
    "  plt.plot(hemi_contrast, label=f\"{HEMIS[i]} hemisphere\")\n",
    "\n",
    "plt.title(\"Contrast of %s - %s\" % (conditions[0], conditions[1]))\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel('Contrast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique network labels\n",
    "network_names = np.unique(region_info[\"network\"])\n",
    "\n",
    "hemi_networks = np.split(region_info[\"network\"], 2)\n",
    "hemi_contrasts = np.split(group_contrast, 2)\n",
    "\n",
    "# Get and plot mean contrast value per network, by hemisphere\n",
    "for hemi, hemi_network, hemi_contrast in zip(HEMIS, hemi_networks, hemi_contrasts):\n",
    "  network_vals = []\n",
    "  for network in network_names:\n",
    "    network_vals.append(hemi_contrast[hemi_network == network].mean())\n",
    "  plt.barh(network_names, network_vals, alpha=.5, label=f\"{hemi} hemi\")\n",
    "\n",
    "plt.axvline(0)\n",
    "plt.xlabel(\"Amplitude difference\")\n",
    "plt.title(f\"Contrast of {conditions[0]} - {conditions[1]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute N of errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'wm'\n",
    "import warnings\n",
    "\n",
    "#ignore by message\n",
    "warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "conditions = ['0bk_err', '0bk_nlr', '2bk_err', '2bk_nlr']\n",
    "conditions2 = ['2bk_nlr']\n",
    "Totalerrorcount = np.zeros([len(subjects),len(conditions)])\n",
    "c = -1\n",
    "for condition in conditions:\n",
    "    c += 1\n",
    "    xx = 0\n",
    "    for subject in subjects:\n",
    "        '''Get number of erroneous trials'''\n",
    "        for id in get_image_ids(task):\n",
    "            task_key = BOLD_NAMES[id - 1]\n",
    "            ev_file = f\"./subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "            x = np.genfromtxt(ev_file)\n",
    "            n_errors = x.shape[0]\n",
    "            Totalerrorcount[[subject], [c]] = n_errors\n",
    "            print(task_key)\n",
    "#np.savetxt('/Users/jonathanorschiedt/Documents/Python/Nerrors.txt',Totalerrorcount, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Totalerrorcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Totalerrorcount, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
