{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To err is human, but where is the error?\n",
    "### Erronous Hamsters group project\n",
    "### Analysis of the HCP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load important libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nistats.hemodynamic_models import glover_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCP_DIR =  \"c:/users/frauke/downloads/hcp_task/\" #change to your data path\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and update necessary files and subject-folders\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_subs_no_error(files_oI, subjects, HCP_DIR, bold_run):\n",
    "    \n",
    "    \"\"\"Get list of subject with 0 errors in at least one condition \n",
    "       in task.\n",
    "\n",
    "    Args:\n",
    "      files_oI (list of strings) : list of condition .txt files to check ()\n",
    "      subjects (list of int) : subjects t\n",
    "      HCP_DIR(str): directory path\n",
    "      bold_run(list): list of bold_runs (\"tfMRI_WM_RL\", \"tfMRI_WM_LR\")      \n",
    "      \n",
    "    Returns:\n",
    "       2x excl_subs_**** (tuple) : list of unique subjects\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "\n",
    "\n",
    "    files_oI = files_oI        #currently ['2bk_err.txt', '2bk_nlr.txt']\n",
    "\n",
    "    excl_subs_tfMRI_WM_RL = [] ##initialize list per task \n",
    "    excl_subs_tfMRI_WM_LR = []\n",
    "    \n",
    "\n",
    "    for t in bold_run: # iterate over task and subjects\n",
    "\n",
    "        for sub in subjects:\n",
    "\n",
    "            path = HCP_DIR + \"/subjects/{}/EVs/{}\".format(sub,t) #set path\n",
    "            files = os.listdir(path)\n",
    "            files = [file for file in files if file in files_oI] # choose files of interest from path\n",
    "\n",
    "            for file in files:\n",
    "                f = np.genfromtxt(path + \"/\" + file)\n",
    "\n",
    "                if f.size == 0: # check for empty files in specific task\n",
    "                    if t == \"tfMRI_WM_RL\":\n",
    "                        excl_subs_tfMRI_WM_RL.append(sub) # append to respective list \n",
    "                    else:\n",
    "                        excl_subs_tfMRI_WM_LR.append(sub)\n",
    "                        \n",
    "    #print(len(list(set(excl_subs_tfMRI_WM_RL)))) ###sanity check\n",
    "    #print(len(list(set(excl_subs_tfMRI_WM_LR))))                    \n",
    "\n",
    "    # return unique subjects per task, hint: if files_oI contains > 1 condition, doubles are possible  \n",
    "    return (list(set(excl_subs_tfMRI_WM_RL)), list(set(excl_subs_tfMRI_WM_LR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(data, subject, bold_run, HCP_DIR, taskcondition):\n",
    "    '''Write txt files from arrays, no matter whether they are empty\n",
    "    Args:\n",
    "        data: data to write in file\n",
    "        subject(int): name of subject\n",
    "        bold_run(str): name of the bold_run/task, e.g. wm_rl\n",
    "        HCP_DIR(str): path which contains data\n",
    "        taskcondition(str): new name of error/condition\n",
    "    '''\n",
    "    \n",
    "    # Check data for size\n",
    "    if data.size < 3: #if an error was made there would be at least one row for onset,duration,amplitude --> 3 int\n",
    "        data = [] #set data equals empty\n",
    "        #write an empty file in subjects folder of the the task with the errorcondition name\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{taskcondition}.txt\",\n",
    "               data, delimiter='\\t')\n",
    "    else: \n",
    "        if data.size == 3: #if data contains 3 columns\n",
    "            data = data.reshape(1,3)\n",
    "        #write file which contains data\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{taskcondition}.txt\",\n",
    "               data, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "        \n",
    "def remove_omission_errors(HCP_DIR, subjects, bold_run, errconditions, nlrconditions):\n",
    "    '''Remove omission errors from _bk_err.txt file.\n",
    "\n",
    "    Args:\n",
    "      HCP_DIR(str) : Path to target directory\n",
    "      subjects(list): list of all subjects whos error files should be updated\n",
    "      bold_run(list): list of task whos error files should be updated\n",
    "      errconditions(list): list of error files which should be updated\n",
    "      nlrconditions(list): list of nullresponse files which are used for comparison between false response and nullresponse\n",
    "    Returns:\n",
    "      New files '_bk_err_fresp.txt' in source directory \n",
    "  '''\n",
    "    for subject in subjects: \n",
    "        for run in bold_run:\n",
    "            #Load error files as numpy arrays \n",
    "            nlrconcat = np.zeros([1,3])\n",
    "            ONSETS = []\n",
    "            #loop through all errorconditions\n",
    "            for i in np.arange(len(errconditions)):\n",
    "                #print('subject {0}, task key {1}, i = {2}'.format(subject, task_key, i))\n",
    "                errorcondition = errconditions[i] #set actual error condition\n",
    "                err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{errorcondition}.txt\") #load corresponding error file\n",
    "                nlr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{nlrconditions[i]}.txt\") #load corresponding null response file\n",
    "                errorname = str(errconditions[i].split('_')[0]+'_fresp') #create the name of sorted error as false response\n",
    "                #if there were zero null responses\n",
    "                if nlr.size == 0:\n",
    "                    #the complete error file contains just false reponses, so the data to write in new file equals the errorfile\n",
    "                    data = err\n",
    "                    #write new error- false reponse file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                    #go to next condition\n",
    "                    continue\n",
    "                # if there was one null reponse AND just one error in the other error file\n",
    "                elif nlr.ndim == 1 and err.ndim == 1:\n",
    "                    #those error file just contains a null response which should be deleted\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[0]\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0)\n",
    "                    #data = []\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                # if there was just one null response but more total errors\n",
    "                elif nlr.ndim == 1 and err.ndim > 1:\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[:,0] # create mask for null reponse in other errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                #else if there were more than one null response and more than one total errors\n",
    "                else:\n",
    "                    onsets = nlr[:,0]\n",
    "                    mask = np.isin(err, nlr)[:,0] #create mask for null response in total errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                ONSETS = np.append(ONSETS, onsets)\n",
    "\n",
    "            all_err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_bk_err.txt\")\n",
    "            errorcondition = 'all_bk_err'\n",
    "            errorname = str(errorcondition.split('_')[0:1]+'_fresp')\n",
    "            #if there was no error at all\n",
    "            if all_err.size == 0:\n",
    "                data = all_err #false response data is empty as well\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                continue\n",
    "            #if amount of total errors equals 1\n",
    "            elif all_err.ndim == 1:\n",
    "                mask = np.isin(all_err[0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "            #if amount of total errors are more than 1\n",
    "            else:\n",
    "                mask = np.isin(all_err[:,0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_X_sorted(HCP_DIR, subject, bold_run):\n",
    "    '''Loads _cor.txt and _err.txt files, concatenates and sorts after time\n",
    "    \n",
    "    Args:\n",
    "    - HCP_DIR (str)  : path of directory \n",
    "    - subject (int)  : current subject \n",
    "    - bold_run (str)  : task/bold run\n",
    "    \n",
    "    Returns: \n",
    "    - X (np.ndarray) : 80x2 ndarray with columns time stamp and errors (1 = error)\n",
    "    '''\n",
    "    #load all responses in two variables\n",
    "    corr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_cor.txt\")\n",
    "    err =  np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_err.txt\")\n",
    "    #stack all onset times nebeneinander\n",
    "    corr = np.hstack((corr[:,0].reshape(-1,1), np.zeros_like(corr[:,0].reshape(-1,1))))\n",
    "    err = np.hstack((err[:,0].reshape(-1,1), np.ones_like(err[:,0]).reshape(-1,1)))\n",
    "    #stack both responses (correct and error) into one array\n",
    "    X = np.vstack((corr, err))\n",
    "    #sort onset times after size\n",
    "    X = X[X[:,0].argsort()]\n",
    "    \n",
    "    return X \n",
    "\n",
    "def diff_stimuli_types(HCP_DIR, subjects, stimuli, bold_run, task_keys):\n",
    "    '''\n",
    "        Takes all stimuli and sort them after stimuli type. Writes onset times of different stimuli type in a several files.\n",
    "        \n",
    "        Args:\n",
    "            HCP_DIR (str): directory of data\n",
    "            subjects(array): array of all subjects\n",
    "            stimuli(list): list of stimuli for which onset files should be written\n",
    "            bold_run(list): list of all run/task for which onset files for different stimuli should be written\n",
    "            task_keys(list): list of tasks for which onset files for different stimuli should be written\n",
    "        saves new stimuli onset files\n",
    "    '''\n",
    "    cue_duration = 2.565\n",
    "    len_block = 10\n",
    "    temp_zeros = np.zeros([80,4])\n",
    "    for subject in subjects:\n",
    "        for run in bold_run[:1]:\n",
    "            X = return_X_sorted(HCP_DIR=HCP_DIR, subject=subject, bold_run=run)\n",
    "            \n",
    "            for task_key in task_keys:\n",
    "                temp_zeros = np.zeros([80,4])\n",
    "                for index, stimulus in enumerate(stimuli):\n",
    "                    stim_name = task_key + stimulus\n",
    "                    stim = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}.txt\")\n",
    "                    #Search for index of start value (min distance between stim onset + \n",
    "                    # cue duration and start of block in onset times of X)\n",
    "                    min_index = np.argmin(np.abs(X[:,0] - (stim[0] + cue_duration)))\n",
    "                    # Create new column of zeros and ones \n",
    "                    temp_zeros[min_index:min_index+len_block, index] = 1\n",
    "                    T = np.hstack((X[:,0].reshape(-1,1), temp_zeros))\n",
    "\n",
    "                    #Create new file with time stamps \n",
    "                    M = np.hstack((X[min_index : min_index+len_block, 0].reshape(-1,1), np.full((len_block,1),2.5),\n",
    "                                   np.full((len_block, 1), 1)))\n",
    "                    # Optional if we want to include the first line \n",
    "                    M = np.vstack((stim, M))\n",
    "                    # Write file in source directory\n",
    "                    np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}_ts.txt\",\n",
    "                               M, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "                \n",
    "                #Write T into file\n",
    "                #np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_{task_key}ts.txt\",\n",
    "                #            T, fmt=['%.3f','%.d', '%.d','%.d', '%.d'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "def load_evs(subject, name, condition):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id-1]\n",
    "    ev_file = f\"{HCP_DIR}subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], np.genfromtxt(ev_file).T))\n",
    "    evs.append(ev)\n",
    "  return evs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, bold_run, remove_mean=False):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"{HCP_DIR}subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "    \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / (TR/10)).astype(int)\n",
    "            \n",
    "        #Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / (TR/10)).astype(int)\n",
    "            \n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        if type(start) == np.ndarray:\n",
    "            frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "        else:\n",
    "              frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "        frames_list.append(np.concatenate(frames))\n",
    "        #print('ev-file is not empty')\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "def condition_frames_iti(run_evs, skip=0):\n",
    "    \"\"\"Identify starting timepoint and duration of an inter-trial-interval corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor((ev[\"onset\"]+2) / (TR/10)).astype(int) \n",
    "            \n",
    "        #Use trial duration to determine how many frames to include for trial        \n",
    "        duration = np.ceil(np.full(ev['duration'].size,0.5) / (TR/10)).astype(int)\n",
    "            \n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        if type(start) == np.ndarray:\n",
    "            frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "        else:\n",
    "            frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "        frames_list.append(np.concatenate(frames))\n",
    "\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev = load_evs(sub, task, '2bk_fresp')\n",
    "ev[0]['duration'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and GLM-Fitting\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_design_matrix(subject,task,conditions,sample_rate):\n",
    "    \"\"\"\n",
    "    Creates a design matrix for given tasks and conditions.\n",
    "    \n",
    "    Args:\n",
    "        task: string of specific task of experiment\n",
    "        conditions: list of conditions, which should be included in design matrix\n",
    "        sample_rate: int of how much the frames should be upsampled\n",
    "    \"\"\"\n",
    "    \n",
    "    #ignore warnings from genfromtxt\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "    #create instances of events and design_matrix\n",
    "    ts = load_single_timeseries(subject,get_image_ids(task)[0])\n",
    "    design_matrix = np.zeros((ts.shape[1]*sample_rate,len(conditions)))\n",
    "    #loop through all conditions and set a 1 in design matrix for every time point condition is fullfilled\n",
    "    for n, cond in enumerate(conditions):\n",
    "        frames= []\n",
    "        ev = [load_evs(subject,task,cond)]\n",
    "        frames = condition_frames_iti(ev[0],skip=0)\n",
    "        #frames = condition_frames(ev[0],skip=0)\n",
    "        design_matrix[frames,n] = 1\n",
    "        \n",
    "    return design_matrix\n",
    "\n",
    "def create_hrf(TR,sampling_rate):\n",
    "    '''\n",
    "        Creates an HRF with specific upsampling rate\n",
    "        \n",
    "        Arg:\n",
    "            tr = repition time of MRT-Scanner\n",
    "            sampling_rate (int) = factor with which hrf should be oversampled on basis of repition time\n",
    "    '''\n",
    "    canon_hrf = glover_hrf(tr=TR, oversampling=sampling_rate)\n",
    "    return canon_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrf_convo(design_matrix,canon_hrf):\n",
    "    \"\"\"\n",
    "    Takes designmatrix upsampled to TR/TReso and convolves it with canonical HRF\n",
    "    Args\n",
    "        design_matrix: C * VolTreso ndarray; indicates closest Tr/TReso timepoint where an event started for specific regressor\n",
    "        bf: basis function - will usually be the hrf; 1D array\n",
    "    Returns\n",
    "        design_matrix_conv: C*Vol array with predicted BOLD response\n",
    "    \"\"\"\n",
    "    design_matrix_pad=np.pad(design_matrix,((0,len(canon_hrf)),(0, 0))) # pad with zeros, so we can fit the hrf also for very late events\n",
    "    design_matrix_conv=np.empty(design_matrix_pad.shape) # prepare convolved design matrix\n",
    "    for C in range(len(design_matrix[1])): # for each condition C\n",
    "\n",
    "        design_matrix_conv[:,C]=np.convolve(design_matrix_pad[:len(design_matrix_pad[0])-(len(canon_hrf)+1),C],canon_hrf) #convolute with design with bf\n",
    "\n",
    "    design_matrix_downsampled=design_matrix_conv[:len(design_matrix_pad[0])-(len(canon_hrf)+2),:] # cut back the padded stuff\n",
    "\n",
    "    design_matrix_downsampled=design_matrix_downsampled[0::10,:] # pick every time-point where we have a volume\n",
    "    constant=np.ones((len(design_matrix_downsampled),1),dtype=int) # generate constant (intercept)\n",
    "    design_matrix_downsampled=np.append(design_matrix_downsampled,constant,axis=1) #append intercept to design matrix\n",
    "    return design_matrix_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some start variables\n",
    "# the shape of the design matrix is based on the script \"DataAnalysis_Errorss\"\n",
    "def test_orthogonality(design_matrix_downsampled, threshold, warn = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes a design martix and computed the correlation between conditions\n",
    "    \n",
    "    input:\n",
    "    design_matrix = m x n np.array with condition columns and time line as a row\n",
    "    \n",
    "    output:\n",
    "    orth_mat = np.array of size n x n indicating the orthogonality of each condition to each other condition\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the output matrix\n",
    "    design_dim = design_matrix_downsampled.shape\n",
    "    orth_mat = np.zeros((design_dim[1], design_dim[1]))\n",
    "    \n",
    "    # two vectors are orthogonal, if their dot product is zero\n",
    "    # loop through all conditions\n",
    "    for condition in range(0,design_dim[1]):\n",
    "        cond_1 = design_matrix_downsampled[:,condition]\n",
    "        # compare to all conditions\n",
    "        for compare in range(0, design_dim[1]):\n",
    "            cond_2 = design_matrix_downsampled[:,compare]\n",
    "            \n",
    "            orth_mat[condition,compare] = cond_1@cond_2\n",
    "                        \n",
    "    # throw a warning for conditions that are not orthogonal\n",
    "    check_pos = []\n",
    "    if warn:\n",
    "        warnings = np.where(orth_mat>threshold)\n",
    "        for pos in range(0,len(warnings[0])):\n",
    "            check_pos = np.zeros(len(warnings))\n",
    "\n",
    "            for dim in range(0,len(warnings)):\n",
    "                check_pos[dim] = warnings[dim][pos]\n",
    "\n",
    "            if not (check_pos[0]== check_pos[1]):\n",
    "                print('Watch out, the matrix is not orthogonal for conditons {}'.format(check_pos))   \n",
    "    \n",
    "    return orth_mat,check_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGLM(design_matrix_downsampled,TimeSeries):\n",
    "    \"\"\"\n",
    "    Takes design matrix and timeseries data and applies MLE to generate beta-estimates\n",
    "    Args: \n",
    "        design_matrix_GLM: C*Vol array with predicted BOLD signal per volume per regressor (MAY NOT CONTAIN 0-SUM VECTORS)\n",
    "        TimeSeries: Region*Vol array with actual BOLD signal averaged across regions\n",
    "    Return:\n",
    "        beta: \n",
    "    \"\"\"\n",
    "    X=design_matrix_downsampled\n",
    "    ts=TimeSeries # get time-series array\n",
    "    #ts=TimeSeries[0][0] # get time-series array\n",
    "    beta=np.empty((np.size(ts,axis=0),np.size(X,axis=1)))\n",
    "    for R in range(np.size(ts,axis=0)):\n",
    "        beta[R,:] = np.linalg.inv(X.T @ X) @ X.T @ ts[R,:].T\n",
    "    return beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level analysis \n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_beta_matrix():\n",
    "    '''\n",
    "        Takes \n",
    "    '''\n",
    "\n",
    "def contrast_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contrasts(beta,convecs):\n",
    "    \"\"\"\n",
    "    Multiplies beta estimates per subject with contrast vectors\n",
    "    Args:\n",
    "        beta: C x R array listing beta estimates for each region\n",
    "        convecs: matrix of contrasts, e.g.: [1 0 0 0 -1]\n",
    "    Returns\n",
    "        Con: Contrast-estimate\n",
    "    \"\"\"\n",
    "    convecs=np.array(convecs).reshape(-1,1)\n",
    "    Con=np.empty((np.size(beta,axis=0),np.size(convecs, axis=1)))\n",
    "    \n",
    "    for c in range(np.size(convecs, axis=1)):\n",
    "        for R in range(np.size(beta, axis=0)):\n",
    "            #print(np.dot(convecs, beta[R,:]))\n",
    "            Con[R,c]=np.dot(convecs[:,c], beta[R,:])\n",
    "    return Con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize basic parameters\n",
    "bold_run = ['tfMRI_WM_RL','tfMRI_WM_LR']\n",
    "bold_ids = [get_image_ids('wm_rl')[0],get_image_ids('wm_lr')[0]]\n",
    "task_keys = ['0bk_','2bk_']\n",
    "stimuli = ['faces', 'body', 'tools', 'places']\n",
    "#create error files with false response and null responses respectively\n",
    "remove_omission_errors(HCP_DIR, subjects,bold_run, ['0bk_err','2bk_err'],['0bk_nlr','2bk_nlr'])\n",
    "#initialize new error files\n",
    "error_files = ['2bk_err_fresp','2bk_nlr']\n",
    "#create lists of subjects with at least one error of each error type in both bold runs\n",
    "subjects_rl, subjects_lr = mask_subs_no_error(error_files, subjects, HCP_DIR, bold_run)\n",
    "#create onset files for each specific stimuli type\n",
    "diff_stimuli_types(HCP_DIR, subjects_rl, stimuli, bold_run[0], task_keys)\n",
    "diff_stimuli_types(HCP_DIR, subjects_lr, stimuli, bold_run[1], task_keys)\n",
    "#make design matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'wm_rl'\n",
    "bold_run = get_image_ids(task)[0]\n",
    "sub = 338\n",
    "design_matrix = make_design_matrix(sub,task,['2bk_err2','2bk_nlr'],10)\n",
    "design_matrix_GLM = hrf_convo(design_matrix, canon_hrf)\n",
    "TimeSeries = load_single_timeseries(sub,bold_run)\n",
    "GLM = fitGLM(design_matrix_GLM,TimeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bold_ids = [get_image_ids('wm_rl')[0],get_image_ids('wm_lr')[0]]\n",
    "bold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-eaafdef39821>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_image_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "get_image_ids['wm_rl'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.,  7.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = 'wm_rl'\n",
    "bold_run = get_image_ids(task)[0]\n",
    "sub = 330\n",
    "design_matrix = make_design_matrix(sub,task,['2bk_fresp','2bk_nlr'],10)\n",
    "np.sum(design_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "canon_hrf = glover_hrf(TR,10)\n",
    "design_matrix_downsampled = hrf_convo(design_matrix, canon_hrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeSeries = load_single_timeseries(sub,bold_run)\n",
    "beta = fitGLM(design_matrix_downsampled,TimeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = calculate_contrasts(beta,[1, -1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
