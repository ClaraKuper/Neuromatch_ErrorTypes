{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To err is human, but where is the error?\n",
    "### Erronous Hamsters group project\n",
    "### Analysis of the HCP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frauke\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: \n",
      "\n",
      " | Starting with Nilearn 0.7.0, all Nistats functionality has been incorporated into Nilearn's stats & reporting modules.\n",
      " | Nistats package will no longer be updated or maintained.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#load important libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nistats.hemodynamic_models import glover_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCP_DIR =  \"c:/users/frauke/downloads/hcp_task/\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and update necessary files and subject-folders\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_subs_no_error(error_files, subjects, HCP_DIR, bold_run):\n",
    "    \n",
    "    \"\"\"Get list of subject with 0 errors in at least one condition \n",
    "       in task.\n",
    "\n",
    "    Args:\n",
    "      files_oI (list of strings) : list of condition .txt files to check ()\n",
    "      subjects (list of int) : subjects t\n",
    "      HCP_DIR(str): directory path\n",
    "      bold_run(list): list of bold_runs (\"tfMRI_WM_RL\", \"tfMRI_WM_LR\")      \n",
    "      \n",
    "    Returns:\n",
    "       2x excl_subs_**** (tuple) : list of unique subjects\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "\n",
    "\n",
    "    files_oI = error_files        #currently ['2bk_err.txt', '2bk_nlr.txt']\n",
    "\n",
    "    excl_subs_tfMRI_WM_RL = [] ##initialize list per task \n",
    "    excl_subs_tfMRI_WM_LR = []\n",
    "    \n",
    "\n",
    "    for t in bold_run: # iterate over task and subjects\n",
    "\n",
    "        for sub in subjects:\n",
    "\n",
    "            path = HCP_DIR + \"/subjects/{}/EVs/{}\".format(sub,t) #set path\n",
    "            files = os.listdir(path)\n",
    "            files = [file for file in files if file in files_oI] # choose files of interest from path\n",
    "\n",
    "            for file in files:\n",
    "                f = np.genfromtxt(path + \"/\" + file)\n",
    "\n",
    "                if f.size == 0: # check for empty files in specific task\n",
    "                    if t == \"tfMRI_WM_RL\":\n",
    "                        excl_subs_tfMRI_WM_RL.append(sub) # append to respective list \n",
    "                    else:\n",
    "                        excl_subs_tfMRI_WM_LR.append(sub)\n",
    "                        \n",
    "    #print(len(list(set(excl_subs_tfMRI_WM_RL)))) ###sanity check\n",
    "    #print(len(list(set(excl_subs_tfMRI_WM_LR))))                    \n",
    "\n",
    "    # return unique subjects per task, hint: if files_oI contains > 1 condition, doubles are possible  \n",
    "    return (list(set(excl_subs_tfMRI_WM_RL)), list(set(excl_subs_tfMRI_WM_LR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  11,\n",
       "  16,\n",
       "  19,\n",
       "  27,\n",
       "  34,\n",
       "  39,\n",
       "  41,\n",
       "  42,\n",
       "  46,\n",
       "  47,\n",
       "  52,\n",
       "  55,\n",
       "  58,\n",
       "  59,\n",
       "  67,\n",
       "  70,\n",
       "  72,\n",
       "  75,\n",
       "  81,\n",
       "  87,\n",
       "  94,\n",
       "  98,\n",
       "  100,\n",
       "  103,\n",
       "  105,\n",
       "  108,\n",
       "  112,\n",
       "  117,\n",
       "  118,\n",
       "  121,\n",
       "  122,\n",
       "  124,\n",
       "  136,\n",
       "  139,\n",
       "  142,\n",
       "  144,\n",
       "  146,\n",
       "  152,\n",
       "  160,\n",
       "  167,\n",
       "  197,\n",
       "  198,\n",
       "  200,\n",
       "  208,\n",
       "  211,\n",
       "  215,\n",
       "  218,\n",
       "  220,\n",
       "  221,\n",
       "  230,\n",
       "  231,\n",
       "  238,\n",
       "  240,\n",
       "  252,\n",
       "  255,\n",
       "  257,\n",
       "  262,\n",
       "  264,\n",
       "  269,\n",
       "  275,\n",
       "  278,\n",
       "  279,\n",
       "  281,\n",
       "  283,\n",
       "  289,\n",
       "  293,\n",
       "  294,\n",
       "  307,\n",
       "  309,\n",
       "  310,\n",
       "  314,\n",
       "  318,\n",
       "  319,\n",
       "  322,\n",
       "  323,\n",
       "  329,\n",
       "  334],\n",
       " [0,\n",
       "  1,\n",
       "  7,\n",
       "  11,\n",
       "  13,\n",
       "  15,\n",
       "  16,\n",
       "  19,\n",
       "  20,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  26,\n",
       "  27,\n",
       "  30,\n",
       "  33,\n",
       "  35,\n",
       "  36,\n",
       "  38,\n",
       "  39,\n",
       "  41,\n",
       "  46,\n",
       "  47,\n",
       "  49,\n",
       "  52,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  68,\n",
       "  72,\n",
       "  73,\n",
       "  75,\n",
       "  80,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  88,\n",
       "  91,\n",
       "  92,\n",
       "  94,\n",
       "  98,\n",
       "  103,\n",
       "  105,\n",
       "  107,\n",
       "  111,\n",
       "  113,\n",
       "  117,\n",
       "  118,\n",
       "  122,\n",
       "  125,\n",
       "  127,\n",
       "  128,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  139,\n",
       "  140,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  146,\n",
       "  149,\n",
       "  150,\n",
       "  154,\n",
       "  158,\n",
       "  160,\n",
       "  163,\n",
       "  167,\n",
       "  169,\n",
       "  171,\n",
       "  173,\n",
       "  176,\n",
       "  179,\n",
       "  181,\n",
       "  182,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  193,\n",
       "  196,\n",
       "  197,\n",
       "  199,\n",
       "  200,\n",
       "  203,\n",
       "  204,\n",
       "  207,\n",
       "  210,\n",
       "  212,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  221,\n",
       "  223,\n",
       "  227,\n",
       "  230,\n",
       "  231,\n",
       "  234,\n",
       "  235,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  242,\n",
       "  243,\n",
       "  249,\n",
       "  252,\n",
       "  254,\n",
       "  255,\n",
       "  257,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  266,\n",
       "  267,\n",
       "  269,\n",
       "  270,\n",
       "  272,\n",
       "  275,\n",
       "  277,\n",
       "  279,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  287,\n",
       "  289,\n",
       "  293,\n",
       "  294,\n",
       "  296,\n",
       "  303,\n",
       "  304,\n",
       "  307,\n",
       "  309,\n",
       "  310,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  319,\n",
       "  321,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  329,\n",
       "  330,\n",
       "  334,\n",
       "  336,\n",
       "  338])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_subs_no_error(['2bk_err.txt', '2bk_nlr.txt'], subjects, HCP_DIR, ['tfMRI_WM_RL','tfMRI_WM_LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(data, subject, bold_run, HCP_DIR, taskcondition):\n",
    "    '''Write txt files from arrays, no matter whether they are empty\n",
    "    Args:\n",
    "        data: data to write in file\n",
    "        subject(int): name of subject\n",
    "        bold_run(str): name of the bold_run/task, e.g. wm_rl\n",
    "        HCP_DIR(str): path which contains data\n",
    "        taskcondition(str): new name of error/condition\n",
    "    '''\n",
    "    \n",
    "    # Check data for size\n",
    "    if data.size < 3: #if an error was made there would be at least one row for onset,duration,amplitude --> 3 int\n",
    "        data = [] #set data equals empty\n",
    "        #write an empty file in subjects folder of the the task with the errorcondition name\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{taskcondition}.txt\",\n",
    "               data, delimiter='\\t')\n",
    "    else: \n",
    "        if data.size == 3: #if data contains 3 columns\n",
    "            data = data.reshape(1,3)\n",
    "        #write file which contains data\n",
    "        np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{taskcondition}.txt\",\n",
    "               data, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "        \n",
    "def remove_omission_errors(HCP_DIR, subjects, bold_run, errconditions, nlrconditions):\n",
    "    '''Remove omission errors from _bk_err.txt file.\n",
    "\n",
    "    Args:\n",
    "      HCP_DIR(str) : Path to target directory\n",
    "      subjects(list): list of all subjects whos error files should be updated\n",
    "      bold_run(list): list of task whos error files should be updated\n",
    "      errconditions(list): list of error files which should be updated\n",
    "      nlrconditions(list): list of nullresponse files which are used for comparison between false response and nullresponse\n",
    "    Returns:\n",
    "      New files '_bk_err_fresp.txt' in source directory \n",
    "  '''\n",
    "    for subject in subjects: \n",
    "        for run in bold_run:\n",
    "            #Load error files as numpy arrays \n",
    "            nlrconcat = np.zeros([1,3])\n",
    "            ONSETS = []\n",
    "            #loop through all errorconditions\n",
    "            for i in np.arange(len(errconditions)):\n",
    "                #print('subject {0}, task key {1}, i = {2}'.format(subject, task_key, i))\n",
    "                errorcondition = errconditions[i] #set actual error condition\n",
    "                err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{errorcondition}.txt\") #load corresponding error file\n",
    "                nlr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{nlrconditions[i]}.txt\") #load corresponding null response file\n",
    "                errorname = str(errconditions[i].split('_')[0]+'_fresp') #create the name of sorted error as false response\n",
    "                #if there were zero null responses\n",
    "                if nlr.size == 0:\n",
    "                    #the complete error file contains just false reponses, so the data to write in new file equals the errorfile\n",
    "                    data = err\n",
    "                    #write new error- false reponse file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                    #go to next condition\n",
    "                    continue\n",
    "                # if there was one null reponse AND just one error in the other error file\n",
    "                elif nlr.ndim == 1 and err.ndim == 1:\n",
    "                    #those error file just contains a null response which should be deleted\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[0]\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0)\n",
    "                    #data = []\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                # if there was just one null response but more total errors\n",
    "                elif nlr.ndim == 1 and err.ndim > 1:\n",
    "                    onsets = nlr[0]\n",
    "                    mask = np.isin(err, nlr)[:,0] # create mask for null reponse in other errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                #else if there were more than one null response and more than one total errors\n",
    "                else:\n",
    "                    onsets = nlr[:,0]\n",
    "                    mask = np.isin(err, nlr)[:,0] #create mask for null response in total errors\n",
    "                    data = np.delete(err, np.where(mask==True), axis=0) #delete masked data and create new (false response) data\n",
    "                    #write false response data in new file\n",
    "                    write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                ONSETS = np.append(ONSETS, onsets)\n",
    "\n",
    "            all_err = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_bk_err.txt\")\n",
    "            errorcondition = 'all_bk_err'\n",
    "            errorname = str(errorcondition.split('_')[0]+'_fresp')\n",
    "            #if there was no error at all\n",
    "            if all_err.size == 0:\n",
    "                data = all_err #false response data is empty as well\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "                continue\n",
    "            #if amount of total errors equals 1\n",
    "            elif all_err.ndim == 1:\n",
    "                mask = np.isin(all_err[0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)\n",
    "            #if amount of total errors are more than 1\n",
    "            else:\n",
    "                mask = np.isin(all_err[:,0], ONSETS) #mask total errors with the onsets of the null responses\n",
    "                data = np.delete(all_err, np.where(mask==True), axis=0) #delete null responses and create new (false response) data\n",
    "                #write false responses in new file\n",
    "                write_files(data, subject, task_key, HCP_DIR, errorname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_X_sorted(HCP_DIR, subject, bold_run):\n",
    "    '''Loads _cor.txt and _err.txt files, concatenates and sorts after time\n",
    "    \n",
    "    Args:\n",
    "    - HCP_DIR (str)  : path of directory \n",
    "    - subject (int)  : current subject \n",
    "    - bold_run (str)  : task/bold run\n",
    "    \n",
    "    Returns: \n",
    "    - X (np.ndarray) : 80x2 ndarray with columns time stamp and errors (1 = error)\n",
    "    '''\n",
    "    #load all responses in two variables\n",
    "    corr = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_cor.txt\")\n",
    "    err =  np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{bold_run}/all_bk_err.txt\")\n",
    "    #stack all onset times nebeneinander\n",
    "    corr = np.hstack((corr[:,0].reshape(-1,1), np.zeros_like(corr[:,0].reshape(-1,1))))\n",
    "    err = np.hstack((err[:,0].reshape(-1,1), np.ones_like(err[:,0]).reshape(-1,1)))\n",
    "    #stack both responses (correct and error) into one array\n",
    "    X = np.vstack((corr, err))\n",
    "    #sort onset times after size\n",
    "    X = X[X[:,0].argsort()]\n",
    "    \n",
    "    return X \n",
    "\n",
    "def diff_stimuli_types(HCP_DIR, subjects, stimuli, bold_run, task_keys):\n",
    "    '''\n",
    "        Takes all stimuli and sort them after stimuli type. Writes onset times of different stimuli type in a several files.\n",
    "        \n",
    "        Args:\n",
    "            HCP_DIR (str): directory of data\n",
    "            subjects(array): array of all subjects\n",
    "            stimuli(list): list of stimuli for which onset files should be written\n",
    "            bold_run(list): list of all run/task for which onset files for different stimuli should be written\n",
    "            task_keys(list): list of tasks for which onset files for different stimuli should be written\n",
    "        saves new stimuli onset files\n",
    "    '''\n",
    "    cue_duration = 2.565\n",
    "    len_block = 10\n",
    "    temp_zeros = np.zeros([80,4])\n",
    "    for subject in subjects[:2]:\n",
    "        for run in bold_run[:1]:\n",
    "            X = return_X_sorted(HCP_DIR=HCP_DIR, subject=subject, bold_run=run)\n",
    "            \n",
    "            for task_key in task_keys:\n",
    "                temp_zeros = np.zeros([80,4])\n",
    "                for index, stimulus in enumerate(stimuli):\n",
    "                    stim_name = task_key + stimulus\n",
    "                    stim = np.genfromtxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}.txt\")\n",
    "                    #Search for index of start value (min distance between stim onset + \n",
    "                    # cue duration and start of block in onset times of X)\n",
    "                    min_index = np.argmin(np.abs(X[:,0] - (stim[0] + cue_duration)))\n",
    "                    # Create new column of zeros and ones \n",
    "                    temp_zeros[min_index:min_index+len_block, index] = 1\n",
    "                    T = np.hstack((X[:,0].reshape(-1,1), temp_zeros))\n",
    "\n",
    "                    #Create new file with time stamps \n",
    "                    M = np.hstack((X[min_index : min_index+len_block, 0].reshape(-1,1), np.full((len_block,1),2.5),\n",
    "                                   np.full((len_block, 1), 1)))\n",
    "                    # Optional if we want to include the first line \n",
    "                    M = np.vstack((stim, M))\n",
    "                    # Write file in source directory\n",
    "                    np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/{stim_name}_ts.txt\",\n",
    "                               M, fmt=['%.3f','%.1f', '%.d'], delimiter='\\t')\n",
    "                \n",
    "                #Write T into file\n",
    "                np.savetxt(f\"{HCP_DIR}/subjects/{subject}/EVs/{run}/all_{task_key}ts.txt\",\n",
    "                            T, fmt=['%.3f','%.d', '%.d','%.d', '%.d'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_stimuli_types(HCP_DIR, subjects,['faces', 'body', 'tools', 'places'],[\"tfMRI_WM_RL\", \"tfMRI_WM_LR\"],['0bk_', '2bk_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "def load_evs(subject, name, condition):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id-1]\n",
    "    ev_file = f\"{HCP_DIR}subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], np.genfromtxt(ev_file).T))\n",
    "    evs.append(ev)\n",
    "  return evs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
    "      or None to load all runs.\n",
    "    concat (bool) : If True, concatenate multiple runs in time\n",
    "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  # Get the list relative 0-based index of runs to use\n",
    "  if runs is None:\n",
    "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
    "  elif isinstance(runs, int):\n",
    "    runs = [runs]\n",
    "\n",
    "  # Get the first (1-based) run id for this experiment \n",
    "  offset = get_image_ids(name)[0]\n",
    "\n",
    "  # Load each run's data\n",
    "  bold_data = [\n",
    "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
    "  ]\n",
    "\n",
    "  # Optionally concatenate in time\n",
    "  if concat:\n",
    "    bold_data = np.concatenate(bold_data, axis=-1)\n",
    "\n",
    "  return bold_data\n",
    "\n",
    "\n",
    "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"{HCP_DIR}subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "    \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        try:\n",
    "            # Determine when trial starts, rounded down\n",
    "            start = np.floor(ev[\"onset\"] / (TR/10)).astype(int)\n",
    "            \n",
    "            #Use trial duration to determine how many frames to include for trial\n",
    "            duration = np.ceil(ev[\"duration\"] / (TR/10)).astype(int)\n",
    "            \n",
    "            # Take the range of frames that correspond to this specific trial\n",
    "            if type(start) == np.ndarray:\n",
    "                frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "            else:\n",
    "                frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "            frames_list.append(np.concatenate(frames))\n",
    "            #print('ev-file is not empty')\n",
    "        except KeyError:\n",
    "            print('ev-file is empty')\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "def condition_frames_iti(run_evs, skip=0):\n",
    "    \"\"\"Identify starting timepoint and duration of an inter-trial-interval corresponding to a given condition in each run.\n",
    "    Args:\n",
    "        run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "        for hemodynamic lag\n",
    "    \n",
    "    Returns:\n",
    "        frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "        \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    for ev in run_evs:\n",
    "        try:\n",
    "            # Determine when trial starts, rounded down\n",
    "            start = np.floor((ev[\"onset\"]+2) / (TR/10)).astype(int) \n",
    "            \n",
    "            #Use trial duration to determine how many frames to include for trial        \n",
    "            duration = np.ceil((np.ones(len(ev['duration']))*0.5) / (TR/10)).astype(int)\n",
    "            \n",
    "            # Take the range of frames that correspond to this specific trial\n",
    "            if type(start) == np.ndarray:\n",
    "                frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "            else:\n",
    "                frames = [start+np.arange(skip,duration)]\n",
    "            \n",
    "            frames_list.append(np.concatenate(frames))\n",
    "            #print('ev-file is not empty')\n",
    "        except KeyError:\n",
    "            print('ev-file is empty')\n",
    "\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and GLM-Fitting\n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_design_matrix(subject,task,conditions,sample_rate):\n",
    "    \"\"\"\n",
    "    Creates a design matrix for given tasks and conditions.\n",
    "    \n",
    "    Args:\n",
    "        task: string of specific task of experiment\n",
    "        conditions: list of conditions, which should be included in design matrix\n",
    "        sample_rate: int of how much the frames should be upsampled\n",
    "    \"\"\"\n",
    "    \n",
    "    #ignore warnings from genfromtxt\n",
    "    warnings.filterwarnings(\"ignore\", message=\"genfromtxt\")\n",
    "    #create instances of events and design_matrix\n",
    "    ts = load_single_timeseries(subject,get_image_ids(task)[0])\n",
    "    design_matrix = np.zeros((ts.shape[1]*sample_rate,len(conditions)))\n",
    "    #loop through all conditions and set a 1 in design matrix for every time point condition is fullfilled\n",
    "    for n, cond in enumerate(conditions):\n",
    "        frames= []\n",
    "        ev = [load_evs(subject,task,cond)]\n",
    "        frames = condition_frames_iti(ev[0],skip=0)\n",
    "        #frames = condition_frames(ev[0],skip=0)\n",
    "        design_matrix[frames,n] = 1\n",
    "    #loop through design matrix columns and delete design matrix if at least one row has no entries\n",
    "    for i in range(len(conditions)):\n",
    "        if np.sum(design_matrix,axis=0)[i]==0:\n",
    "            design_matrix = []\n",
    "            print(subject,' has not made enough errors')\n",
    "            break\n",
    "    return design_matrix\n",
    "\n",
    "def create_hrf(tr,sampling_rate):\n",
    "    '''\n",
    "        Creates an HRF with specific upsampling rate\n",
    "        \n",
    "        Arg:\n",
    "            tr = repition time of MRT-Scanner\n",
    "            sampling_rate (int) = factor with which hrf should be oversampled on basis of repition time\n",
    "    '''\n",
    "    #creates an hrf\n",
    "    canon_hrf = glover_hrf(tr=tr, oversampling=sampling_rate)\n",
    "    \n",
    "    # Define timepoints corresponding to HRF\n",
    "    t_hrf = np.linspace(0, np.floor(32/tr).astype(int), np.round(32/tr * sampling_rate).astype(int), endpoint=False)\n",
    "    \n",
    "    return canon_hrf, t_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrf_convo(design_matrix,bf):\n",
    "    \"\"\"\n",
    "    Takes designmatrix upsampled to TR/TReso and convolves it with canonical HRF\n",
    "    \n",
    "    Args\n",
    "    design_matrix: C * VolTreso ndarray; indicates closest Tr/TReso timepoint where an event started for specific regressor\n",
    "    bf: basis function - will usually be the hrf; 1D array\n",
    "\n",
    "    Returns\n",
    "    design_matrix_conv: CVol array with predicted BOLD response\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        design_matrix_pad=np.pad(design_matrix,((0,len(bf)),(0, 0))) # pad with zeros, so we can fit the hrf also for very late events\n",
    "        design_matrix_conv=np.empty(design_matrix_pad.shape) # prepare convolved design matrix\n",
    "    \n",
    "        for C in range(len(design_matrix[1])): # for each condition C\n",
    "            design_matrix_conv[:,C]=np.convolve(design_matrix_pad[:len(design_matrix_pad[0])-(len(bf)+1),C],bf) #convolute with design with bf\n",
    "\n",
    "        design_matrix_downsampled=design_matrix_conv[:len(design_matrix_pad[0])-(len(bf)+2),:] # cut back the padded stuff\n",
    "\n",
    "        design_matrix_downsampled=design_matrix_downsampled[0::10,:] # pick every time-point where we have a volume\n",
    "    \n",
    "        return design_matrix_downsampled\n",
    "    except ValueError:\n",
    "        print('subject has no design matrix')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGLM(design_matrix_GLM,TimeSeries):\n",
    "    \"\"\"\n",
    "    Takes design matrix and timeseries data and applies MLE to generate beta-estimates\n",
    "    Args: \n",
    "        design_matrix_GLM: C*Vol array with predicted BOLD signal per volume per regressor (MAY NOT CONTAIN 0-SUM VECTORS)\n",
    "        TimeSeries: Region*Vol array with actual BOLD signal averaged across regions\n",
    "    Return:\n",
    "        beta: \n",
    "    \"\"\"\n",
    "    X=design_matrix_GLM\n",
    "    ts=TimeSeries # get time-series array\n",
    "    beta=np.empty((np.size(TimeSeries,axis=0),np.size(X,axis=1)))\n",
    "    for R in range(np.size(ts,axis=0)):\n",
    "        beta[R,:] = np.linalg.inv(X.T @ X) @ X.T @ ts[R,:].T\n",
    "    return beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some start variables\n",
    "# the shape of the design matrix is based on the script \"DataAnalysis_Errorss\"\n",
    "mat_shape = (4050, 2)\n",
    "design_matrix = np.array([[1, 0, 1, 0, 0, 0],\n",
    "                [0, 1, 0, 1, 0 ,0],\n",
    "                [0, 0, 0, 0, 1, 1],\n",
    "                [1, 1, 1, 1, 0, 0]])\n",
    "\n",
    "def test_orthogonality(design_matrix, warn = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes a design martix and computed the correlation between conditions\n",
    "    \n",
    "    input:\n",
    "    design_matrix = m x n np.array with condition columns and time line as a row\n",
    "    \n",
    "    output:\n",
    "    orth_mat = np.array of size n x n indicating the orthogonality of each condition to each other condition\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the output matrix\n",
    "    design_dim = design_matrix.shape\n",
    "    orth_mat = np.zeros((design_dim[1], design_dim[1]))\n",
    "    \n",
    "    # two vectors are orthogonal, if their dot product is zero\n",
    "    # loop through all conditions\n",
    "    for condition in range(0,design_dim[1]):\n",
    "        cond_1 = design_matrix[:,condition]\n",
    "        # compare to all conditions\n",
    "        for compare in range(0, design_dim[1]):\n",
    "            cond_2 = design_matrix[:,compare]\n",
    "            \n",
    "            orth_mat[condition,compare] = cond_1@cond_2/design_dim[1]\n",
    "                        \n",
    "    # throw a warning for conditions that are not orthogonal\n",
    "    if warn:\n",
    "        warnings = np.where(orth_mat>0)\n",
    "        for pos in range(0,len(warnings[0])):\n",
    "            check_pos = np.zeros(len(warnings))\n",
    "\n",
    "            for dim in range(0,len(warnings)):\n",
    "                check_pos[dim] = warnings[dim][pos]\n",
    "\n",
    "            print(check_pos)\n",
    "            if not (check_pos[0]== check_pos[1]):\n",
    "                print('Watch out, the matrix is not orthogonal for conditons {}'.format(check_pos))   \n",
    "    \n",
    "    return orth_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level analysis \n",
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_average(timeseries_data, ev, skip=0):\n",
    "  \"\"\"Take the temporal mean across frames for a given condition.\n",
    "\n",
    "  Args:\n",
    "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
    "    ev (dict or list of dicts): Condition timing information\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    avg_data (1D array): Data averagted across selected image frames based\n",
    "    on condition timing\n",
    "\n",
    "  \"\"\"\n",
    "  # Ensure that we have lists of the same length\n",
    "  if not isinstance(timeseries_data, list):\n",
    "    timeseries_data = [timeseries_data]\n",
    "  if not isinstance(ev, list):\n",
    "    ev = [ev]\n",
    "  if len(timeseries_data) != len(ev):\n",
    "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
    "\n",
    "  # Identify the indices of relevant frames\n",
    "  frames = condition_frames(ev)\n",
    "\n",
    "  # Select the frames from each image\n",
    "  selected_data = []\n",
    "  for run_data, run_frames in zip(timeseries_data, frames):\n",
    "    selected_data.append(run_data[:, run_frames])\n",
    "\n",
    "  # Take the average in each parcel\n",
    "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
    "\n",
    "  return avg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize basic parameters\n",
    "bold_run = ['tfMRI_WM_RL','tfMRI_WM_LR']\n",
    "bold_ids = [get_image_ids('wm_rl')[0],get_image_ids('wm_lr')[0]]\n",
    "task_keys = ['0bk_','2bk_']\n",
    "stimuli = ['faces', 'body', 'tools', 'places']\n",
    "#create error files with false response and null responses respectively\n",
    "remove_omission_errors(HCP_DIR, subjects,bold_run, ['0bk_err','2bk_err'],['0bk_nlr','2bk_nlr'])\n",
    "#initialize new error files\n",
    "error_files = ['2bk_err_fresp','2bk_nlr']\n",
    "#create lists of subjects with at least one error of each error type in both bold runs\n",
    "subjects_rl, subjects_lr = mask_subs_no_error(error_files, subjects, HCP_DIR, bold_run)\n",
    "#create onset files for each specific stimuli type\n",
    "diff_stimuli_types(HCP_DIR, subjects_rl, stimuli, bold_run[0], task_keys)\n",
    "diff_stimuli_types(HCP_DIR, subjects_lr, stimuli, bold_run[1], task_keys)\n",
    "#make design matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'wm_rl'\n",
    "bold_run = get_image_ids(task)[0]\n",
    "sub = 338\n",
    "design_matrix = make_design_matrix(sub,task,['2bk_err2','2bk_nlr'],10)\n",
    "design_matrix_GLM = hrf_convo(design_matrix, canon_hrf)\n",
    "TimeSeries = load_single_timeseries(sub,bold_run)\n",
    "GLM = fitGLM(design_matrix_GLM,TimeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bold_ids = [get_image_ids('wm_rl')[0],get_image_ids('wm_lr')[0]]\n",
    "bold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-eaafdef39821>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_image_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "get_image_ids['wm_rl'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
